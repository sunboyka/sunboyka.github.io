<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>HDFS读写流程</title>
      <link href="/2023/02/22/HDFS%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B/"/>
      <url>/2023/02/22/HDFS%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="HDFS写数据流程"><a href="#HDFS写数据流程" class="headerlink" title="HDFS写数据流程"></a>HDFS写数据流程</h1><img src="/2023/02/22/HDFS%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B/09.PNG" class=""><p>客户端上传一个200M的文件到HDFS集群</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">hdfs客户端生成一个客户端连接对象（DistrbutedFileSystem）</span><br><span class="line">1.客户端连接对象向NameNode发起上传请求，告诉NameNode上传文件的位置</span><br><span class="line">NameNode拿到上传请求后会进行校验</span><br><span class="line">--1.上传文件是否存在或合法</span><br><span class="line">--2.上传文件操作用户的权限校验</span><br><span class="line"></span><br><span class="line">2.如果校验成功 则响应可以上传文件</span><br><span class="line"></span><br><span class="line">3.客户端连接对象请求上传第一个Blok(数据块（0-128M）)，请求元数据返回DataNode</span><br><span class="line">Hdfs会将文件做一个物理上的切分 切分为一个个数据块 一个数据块的大小是128M</span><br><span class="line"></span><br><span class="line">4.NanaNode返回dn1 dn2 dn3 节点 表示采用这三个节点存储数据 </span><br><span class="line">为什么选三台：因为有3个副本数据 每个副本存储在不同机器上 有多少副本数 它就返回多少节点</span><br><span class="line">这三台是怎么返回的:hadoop有一个默认策略来返回节点 </span><br><span class="line">NameNode返回三个节点 其实就是返回的它们的访问地址 客户端拿到后就能与它们通信了</span><br><span class="line"></span><br><span class="line">5.Hdfs客户端声明一个输出流 建立传输通道（客户端与各个节点的通道）</span><br><span class="line">怎么建立通呢：就近原则</span><br><span class="line">客户端先与与最近的一台请求建立通道 最近的这台机器在依次和后面的机器请求建立传输通道</span><br><span class="line"></span><br><span class="line">6.请求完成后 最后一台机器在依次向前面的机器进行应答 </span><br><span class="line">请求 应答的完成 表示传输通道的建立成功</span><br><span class="line"></span><br><span class="line">7.通道建立完成后 开始真正的传输数据</span><br><span class="line">hdfs客户端将128M的数据传输到三台不同的机器上</span><br><span class="line">但是在真正的传输中 传输的单位是packet</span><br><span class="line">一个packet是64kb的大小</span><br><span class="line"></span><br><span class="line">8.第一个块传输完成后 转到第3步 继续完成其它块的传输</span><br><span class="line">但是这时Namenode节点返回的不一定是之前三个</span><br><span class="line">数据块都是独立的自由存放 NameNode上都会有记录的</span><br></pre></td></tr></table></figure><p>HDFS的数据流</p><pre><code>1. HDFS写数据流程   -- HDFS根据请求返回DataNode的节点的策略？-- 机架感知      -- 如果当前Client所在机器有DataNode节点，那就返回当前机器DN1,否则从集群中随机一台。      -- 根据第一台机器的位置，然后再其他机架上随机一台，在第二台机器所在机架上再随机一台。      -- 以上策略的缘由：为了提高数据的可靠性，同时一定程度也保证数据传输的效率！         -- 客户端建立传输通道的时候如何确定和哪一台DataNode先建立连接？-- 网络拓扑      -- 找离client最近的一台机器先建立通道。         -- Client为什么是以串行的方式建立通道？      -- 本质上就是为了降低client的IO开销         -- 数据传输的时候如何保证数据成功？（了解）      -- 采用了ack回执的策略保证了数据完整成功上传。</code></pre><p>ack回执的策略：  </p><p>  传输数据时，是按一个个Blok(128M的数据块)传输的，而这些数据块会分为一个个64k大小的packet, 而一个packet又被分为一个个512B大小的chunk 在加上4B的校验字节 他其实是516B</p><p>当一个chunk装满了 它会被放进一个dataQueue队列中，dataQueue中放的是一个个chunk</p><p>数据会从dataQueue移动到ackQueue队列中  再从ackQueue中放到具体的服务器 最后组成一个packet  而多个packet组成一个数据块</p><p>如果上传失败 数据会从ackQueue 移回到 dataQueue中等待通道连接正常</p><p>ackQueue就像一个中间变量 如果通道连接正常 数据就从 ackQueue发送到服务器  如果传输通道连接失败 数据就会从 ackQueue移回dataQueue 就是一种回退机制</p><h1 id="机架感知和网络拓扑"><a href="#机架感知和网络拓扑" class="headerlink" title="机架感知和网络拓扑"></a>机架感知和网络拓扑</h1><h2 id="网络拓扑"><a href="#网络拓扑" class="headerlink" title="网络拓扑"></a>网络拓扑</h2><p>网络拓跋表达的意思是两个机器之间的距离</p><p>HDFS客户端找最近的机器建立传输通道 就是通过网络拓扑来完成的</p><img src="/2023/02/22/HDFS%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B/a1-16487914800331.PNG" class=""><p>最外面一层是一个机房得路由器</p><p>机房中有不同集群的路由器</p><p>而集群中有不同的机架</p><p>机架上有多台电脑</p><p>例如，假设有数据中心d1机架r1中的节点n1。该节点可以表示为/d1/r1/n1。利用这种标记，这里给出四种距离描述</p><h2 id="机架感知"><a href="#机架感知" class="headerlink" title="机架感知"></a>机架感知</h2><p>机架感知是hadoop选择副本的原则</p><img src="/2023/02/22/HDFS%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B/12.PNG" class=""><p>总结：首先按就近原则选一个副本 其次是跨机架随机选两台机器来保存副本</p><p>跨机架是为了数据的安全性</p><p>但不易多次跨机架</p><p>在保存安全的基础上也要保证性能 </p><h1 id="6-HDFS读数据流程"><a href="#6-HDFS读数据流程" class="headerlink" title="6.HDFS读数据流程"></a>6.HDFS读数据流程</h1><img src="/2023/02/22/HDFS%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B/21.PNG" class=""><h1 id="NN和2NN的工作机制"><a href="#NN和2NN的工作机制" class="headerlink" title="NN和2NN的工作机制"></a>NN和2NN的工作机制</h1><p>思考：NameNode中的元数据是存储在哪里的？</p><p>首先，我们做个假设，如果存储在NameNode节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。因此产生在磁盘中备份元数据的FsImage。</p><p>这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦NameNode节点断电，就会产生数据丢失。因此，引入Edits文件(只进行追加操作，效率很高)。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中。这样，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据。</p><p>但是，如果长时间添加数据到Edits中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行FsImage和Edits的合并，如果这个操作由NameNode节点完成，又会效率过低。因此，引入一个新的节点SecondaryNamenode，</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"> 1. 元数据信息要保存在哪？</span><br><span class="line">   </span><br><span class="line">   1.1 保存到磁盘 </span><br><span class="line">       -- 不足：读写速度慢 效率低！</span><br><span class="line">   1.2 保存内存</span><br><span class="line">       -- 不足：数据不安全</span><br><span class="line">   1.3 最终的解决方案： 磁盘 + 内存</span><br><span class="line">   </span><br><span class="line">2. 内存中的元数据和磁盘中的元数据如何进行同步。（元数据的维护策略）</span><br><span class="line">   </span><br><span class="line">      当我们对元数据进行操作的时候，首先在内存进行合并，其次还要把相关</span><br><span class="line">操作记录追加到edits编辑日志文件中，在满足一定条件下，将edits文件中的</span><br><span class="line">记录合并到元数据信息文件中 fsimage 。</span><br><span class="line"></span><br><span class="line">3. 谁负责对NN的元数据信息进行合并？</span><br><span class="line">      2NN主要负责对NN的元数据进行合并，当满足一定条件的下，2NN会检测本地时间，每隔</span><br><span class="line">一个小时会主动对NN的edits文件和fsimage文件进行一次合并。合并的时候，首先会通知</span><br><span class="line">NN,这时候NN就会停止对正在使用的edits文件的追加，同时会新建一个新的edits编辑日</span><br><span class="line">志文件，保证NN的正常工作。接下来 2NN会把NN本地的fsimage文件和edits编辑日志拉取</span><br><span class="line">        2NN的本地，在内存中对二者进行合并，最后产生最新fsimage文件。把最新的fsimage文件再</span><br><span class="line">        发送给NN的本地。注意还有一个情况，当NN的edits文件中的操作次数累计达到100万次，即便</span><br><span class="line">        还没到1小时，2NN（每隔60秒会检测一次NN方的edits文件的操作次数）也会进行合并。</span><br><span class="line">    2NN 也会自己把最新的fsimage文件备份一份。</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据开发 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HDFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>初识HIVE</title>
      <link href="/2023/02/21/hive01/"/>
      <url>/2023/02/21/hive01/</url>
      
        <content type="html"><![CDATA[<h1 id="Hive概念"><a href="#Hive概念" class="headerlink" title="Hive概念"></a>Hive概念</h1><h2 id="什么是Hive"><a href="#什么是Hive" class="headerlink" title="什么是Hive"></a>什么是Hive</h2><p>hive:由Facebook开源用于解决海量结构化日志的数据统计工具。</p><p>hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能</p><p>Hive本质：将HQL转化成MapReduce程序</p><img src="/2023/02/21/hive01/12.PNG" class=""><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.hive处理的数据存储在HDFs</span><br><span class="line">2.hive分析数据底层的实现是MapReduce</span><br><span class="line">3.执行程序运行在Yaun上</span><br></pre></td></tr></table></figure><h2 id="Hive的优缺点"><a href="#Hive的优缺点" class="headerlink" title="Hive的优缺点"></a>Hive的优缺点</h2><p>1.优点</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1.操作结构采用类SQL语法，提供快速开发的能力（简单，容易上手）</span><br><span class="line">2.避免了去写MapReduce减少了开发人员的学习成本</span><br><span class="line">3.Hive的执行延迟比较高，因此Hive常用于数据分析，对实时性要求不高的场合</span><br><span class="line">4.Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数</span><br><span class="line">5.Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数</span><br></pre></td></tr></table></figure><p>2.缺点</p><p>–1.Hive的HQL表达能力有限</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">（1）迭代式算法无法表达</span><br><span class="line">（2）数据挖掘方面不擅长，由于MapReduce数据处理流程的限制，效率更高的算法却无法实现。</span><br></pre></td></tr></table></figure><p>–2.HIve的效率比较低</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">（1）Hive自动生成的MapReduce作业，通常情况下不够智能化</span><br><span class="line">（2）Hive调优比较困难，粒度较粗</span><br></pre></td></tr></table></figure><h2 id="Hive架构原理"><a href="#Hive架构原理" class="headerlink" title="Hive架构原理"></a>Hive架构原理</h2><img src="/2023/02/21/hive01/13.PNG" class=""><p>1.用户接口：Client</p><p>CLI（command-line interface）、JDBC/ODBC(jdbc访问hive)、WEBUI（浏览器访问hive）</p><p>2.元数据：Metastore</p><p>元数据包括：表名、表所属的数据库（默认是default）、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等；</p><p>默认存储在自带的derby数据库中，推荐使用MySQL存储Metastore</p><p>3.Hadoop</p><p>使用HDFS进行存储，使用MapReduce进行计算</p><p>4.驱动器:Driver</p><p>（1）解析器（SQL Parser）：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误。</p><p>（2）编译器（Physical Plan）：将AST编译生成逻辑执行计划。</p><p>（3）优化器（Query Optimizer）：对逻辑执行计划进行优化。</p><p>（4）执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是MR/Spark。</p><img src="/2023/02/21/hive01/14.PNG" class=""><p>Hive通过给用户提供的一系列交互接口，接收到用户的指令(SQL)，使用自己的Driver，结合元数据(MetaStore)，将这些指令翻译成MapReduce，提交到Hadoop中执行，最后，将执行返回的结果输出到用户交互接口</p><h2 id="Hive和数据库的比较"><a href="#Hive和数据库的比较" class="headerlink" title="Hive和数据库的比较"></a>Hive和数据库的比较</h2><p>Hive不是数据库！</p><p>  由于 Hive 采用了类似SQL 的查询语言 HQL(Hive Query Language)，因此很容易将 Hive 理解为数据库。其实从结构上来看，Hive 和数据库除了拥有类似的查询语言，再无类似之处。本文将从多个方面来阐述 Hive 和数据库的差异。数据库可以用在 Online 的应用中，但是Hive 是为数据仓库而设计的，清楚这一点，有助于从应用角度理解 Hive 的特性</p><h3 id="查询语言"><a href="#查询语言" class="headerlink" title="查询语言"></a>查询语言</h3><p>由于SQL被广泛的应用在数据仓库中，因此，专门针对Hive的特性设计了类SQL的查询语言HQL。熟悉SQL开发的开发者可以很方便的使用Hive进行开发</p><h3 id="数据更新"><a href="#数据更新" class="headerlink" title="数据更新"></a>数据更新</h3><p>由于Hive是针对数据仓库应用设计的，而数据仓库的内容是读多写少的。因此，Hive中不建议对数据的改写，所有的数据都是在加载的时候确定好的。而数据库中的数据通常是需要经常进行修改的，因此可以使用 INSERT INTO … VALUES 添加数据，使用 UPDATE … SET修改数据。</p><h3 id="执行延迟"><a href="#执行延迟" class="headerlink" title="执行延迟"></a>执行延迟</h3><p>Hive 在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。另外一个导致 Hive 执行延迟高的因素是 MapReduce框架。由于MapReduce 本身具有较高的延迟，因此在利用MapReduce 执行Hive查询时，也会有较高的延迟。相对的，数据库的执行延迟较低。当然，这个低是有条件的，即数据规模较小，当数据规模大到超过数据库的处理能力的时候，Hive的并行计算显然能体现出优势。</p><h3 id="数据规模"><a href="#数据规模" class="headerlink" title="数据规模"></a>数据规模</h3><p>由于Hive建立在集群上并可以利用MapReduce进行并行计算，因此可以支持很大规模的数据；对应的，数据库可以支持的数据规模较小。</p>]]></content>
      
      
      <categories>
          
          <category> 大数据开发 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HIVE框架 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>冰山的阴影 --有感</title>
      <link href="/2022/01/27/%E5%86%B0%E5%B1%B1%E7%9A%84%E9%98%B4%E5%BD%B1/"/>
      <url>/2022/01/27/%E5%86%B0%E5%B1%B1%E7%9A%84%E9%98%B4%E5%BD%B1/</url>
      
        <content type="html"><![CDATA[<p>&emsp;&emsp;一个男人除了工作就是独自旅行，常用联系人只有母亲和亲妹妹，葬礼没人参加，仅有的两箱遗物被随便卖掉，一个人的一生和他的阅历就这样被摆在跳蚤市场上出售，典型的孤独死。<br>&emsp;&emsp;在孤独成为常态的当下，类似的人生只会越来越多，行色匆匆的世界没人在意一个普通单身男人的死亡，除了本片导演安蒂。一次偶然，安蒂在跳蚤市场淘到两箱8mm胶片，每卷4分钟，总共20小时。<br>&emsp;&emsp;浓雾的船头，晴天的群山，自由的飞鸟，欢乐的人群，异国风情的建筑，还有一张明信片，写着“来自世界尽头的问候”，落款“我”。安蒂一下子被吸引了，他问了装胶片的信封上的地址，得知胶片的主人已经去世，也没有在世的直系亲属，而关于胶片主人的生平，更是不得而知，就好像他凭空消失，只留下这些胶片一样。“我想记住你，我想知道你为什么旅行到世界的尽头，还用心的记录下这些图像”，安蒂几经辗转，找到了胶片的主人，也就是考文在外地的表妹，又前前后后历时六年，拼凑出来考文的人生轨迹。<br>&emsp;&emsp;考文在1911年出生于芬兰一座港口城市科特卡，是洗衣工和锅炉工的儿子，有个小四岁的妹妹，他的童年在贫乏和不安中度过，为了补贴家用，他需要经常打包本就不多的行李，到叔叔的农村帮忙，据他在农场的堂弟说，他每次听到马车声，总是最先跑到栅栏那儿开门，因为第一个孩子能从司机手里得到一枚硬币，等他到了青年时期，就已经对未来有所打算，他会在放学后学做制图员，姨妈问他有没有女朋友，他说还没遇到那个人，对他来说最重要的只有母亲和妹妹。但，他会孤独。<br>&emsp;&emsp;二战时他在航空站当机械工，看到报纸上的征友启事，就写了篇准备寄过去，[ 我不高也不矮，一米七四，穿43码的鞋，虽然我不清楚腰围，不过普通的卷尺肯定够我量了，我爱看电影，也喜欢吃煎饼，华尔兹是我最喜欢的舞种，只可惜我不在年轻，我31岁了还没结婚，愿你有个不受空袭困扰的愉快假期 ]，满怀期待，特别可爱。但因为信件暴露了军队的地址和代码，并没有被送出去，他还因此受到了处分，那之后，他就再也没有交友的痕迹。<br>&emsp;&emsp;战后，他进了玻璃厂，用攒下的钱买了一栋小房子，和母亲还有妹妹住了进去，但一个夏天后，他就偷偷打包行李离开了，还偷偷买了个很贵的相机，他跑到船上当了船员，他没学过航海专业知识，就给发动机添煤，只要能待在船上，什么活他都愿意干。那年，他42岁。<br>&emsp;&emsp;刚上船的三个月，出于兴奋，他给母亲和妹妹写了50多页的信，[这趟旅程把我带到了葡萄牙，我在一个集市还是广场的地方下来车，我的眼睛和灵魂都感到天旋地转，强烈的阳光和灰白的建筑刺激了我的眼睛，所有我能看到和听到，闻到和感觉到的都刺激到了我的灵魂，我这个乡下土包子差点都吓到掉头回去了，但不知为什么，我觉得这一切就是应有的样子]。信里充满热情，他终于体会到了一直梦寐以求的东西，这趟旅程之后，他专门去学了一年的机械，带着新技术去了一条新船，这条新船把他带去了更远的地方，极地的冰山区。这儿浓雾弥漫，海鸥都不敢过来，他还遇上了持续一周的恶劣天气，每天早晨，他都会听到广播里的暴风警报，就像上帝喊了预备开始一般，然后大海会变得越来越汹涌，最后变成狂暴的怒吼。【水之于云 犹如雪之于冰 它们凝聚着这浩海的力量】他这么描述到，但他似乎一点都不害怕，还在信里调皮地嘱咐妹妹，下次坐船记得买保险。<br>&emsp;&emsp;这场暴风雨持续了一周，在天气转好后地第一个清晨，他早早起床看日出，[一周后地清晨，你站在甲板上，握着相机，热切地注视着地平线 ]。也许对他而言，一切都可以算作置身事外地体验。再后来，他去的地方越来越多，比如超级大都市曼哈顿，[曼哈顿的摩天大楼，仿佛一片奇异的植物簇群，慢慢从灰雾中显现出来，如此庞大]，[发热的红色信号灯警示地闪着”不要过来，不要过来”，不久后绿灯又像妖娆女子一样地诱惑者你“来这边，过来嘛”]，他还去了埃及，[ 这里地风景就跟棕榈树一样都很有异国情调 ]，[ 我的脑袋被太阳灼烧得像个桑拿炉，一定要待在阴影下 ]，他对金字塔异常感兴趣，这能让他想起几千年前得劳工，为修筑如此伟大得建筑洒下得汗水。他的精神总爱飞到很远的地方，他问当地人如何快速到达塔顶，得到的答案是[从角落上去]，他就在石阶上一步步向上跳，中途他挺下来喘口气，顺便看风景。这是他头一次穿着衣服待在这样热的地方，让他觉得[ 空气在发光 喧嚣尽散 就像一切都停止了 ]。<br>&emsp;&emsp;旅行多了，他喜欢上了待在阴影里，那里很凉爽，于是那时他开始想，在冰山的阴影下会是什么感觉，并把这作为梦想的旅行地。他就这么去了很多地方，也体会到了很多，但不管他去哪里，总是独占一人，他信里唯一跟[同伴]有关的表述是路遇的一位警察，那位警察把他从船里拉出来，带他去逛街，给他买街边的水果，虽然言语不通，但警察还是把他带回家吃饭，他因此有幸见到了警察温柔的妻子和两个可爱的孩子，一个四岁的小男孩和一个婴儿。他在信里仔细描述着警察家里的一切，甚至让人怀疑有一瞬间，他这个孑然一身的家伙，被这份温馨安定的气息所吸引了，又甚至，他看着这家人脸上幸福的笑容，也想结个婚成个家了，他四十多了还是单身，当人们问他，为何不娶妻，他会说家里已经有两个好女人了。<br>&emsp;&emsp;他给母亲和妹妹在市里买了新公寓，那就是他的家。他经常在信里想象[母亲正在做香喷喷的饭菜 妹妹在那里躺着晒太阳]，而他的父亲只身留在旧公寓，后来中风去世了。又过了几年，他做到了高级船舶工程师，位置只在船长之下，这份工作很自由，很合他的心意，他总是一靠港就带着相机出发，让船员们自己去工作，他就这么做到了退休。本该享受天伦之乐的时候，妹妹却因癌症去世了，他在家独占照顾母亲，给母亲拍照。照顾了四年，母亲也去世了，他就又开始环游世界，在他67岁的时候。只是这一次，他没有人写信，就自己给自己寄明信片，他把之前没去过的地方去了个遍，亚洲，大洋洲，人文的，自然的，安全的，惊险的，他都看过了。去泰姬陵的时候，他实在想找人分享，就把明信片寄给了夏日度假屋的邻居，在去了失落的印加城市后，他的足迹已经遍布六大洲，但他还不满足，他还没看到冰山的阴影，那年，他77岁。<br>&emsp;&emsp;他坐上了阿根廷海军补给舰，但刚到能看到南极洲海岸的地方，舰船失事了，他虽然被成功救起来，但路上拍的胶片都丢了。死里逃生并没有阻止他的脚步，四年后，他又踏上另一段旅程，带着一台全新的摄像机，这一次，他如愿以偿，他给自己寄了封明信片写着，[ 来自世界尽头的问候 ]，落款[ 我 ]。他就这样走遍了全世界，2001年，90岁的他安静离世，和母亲还有妹妹葬在一起，只有木屋协会的几个人出席了他的葬礼，一个二十年没见的姨妈继承了他的遗产。<br>&emsp;&emsp;导演安蒂本以为考文的一生就这样了，但有天他发现了一张剪报，那是考文离世前6个月的报纸，标题[ 就是个天堂 ],作者[ 太空漫客丹尼斯 ]，安蒂还找到一个装着写有密密麻麻字的信封，看来考文当时在研究一些天文概念，其中一个概念是爱因斯坦的罗森桥，或者去到所在宇宙远处的一个点[ 虫洞 ]。看来，考文还想去到更远的地方。<br>&emsp;&emsp;“为什么你去旅行和见识新鲜事物，还有去捕捉的欲望是如此强烈，你真是雄心勃勃”，安蒂并不能解答这个问题，但看遍了考文留下的影像和信之后，他有一件事可以确定，[ 即使你从没遇上那个对的人 但你找到了此生所追求的事物 ]，他始终记得考文信中的一段话，[ 一个周六早晨我蹑手蹑脚走到了甲板上 看到太阳从远处的海岸线上升起 在一公里外的半山腰上点亮了整个城市 我深吸一口气 感觉在空气和眼前的景色中出现了一种奇怪的氛围 此时我觉得自己就像抵达了另一个世界 ]。</p>]]></content>
      
      
      <categories>
          
          <category> 感想 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随感 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>随感</title>
      <link href="/2021/12/03/shuigan1/"/>
      <url>/2021/12/03/shuigan1/</url>
      
        <content type="html"><![CDATA[<p>&emsp;&emsp;现在才发现，自己内心最焦虑的那段时光才是人生最有力量的阶段。因为焦虑的根源并不是生活本身，而是那种迫切想要改变现状的内驱力带来的精神”灼烧“感，人生就是逆熵而行，生活从来都不容易，当觉得容易的时候，一定有人在替你承担属于你的那份不容易。<br>&emsp;&emsp;可能你又荒废了一天，又拖延了一天，但只要这种”灼烧“还存在，你的血就没凉，你就还有无限可能，人不是慢慢变老的，而是一瞬间变老的。如果有一天你觉得麻木了，稳定了，岁月静好了，可能一眨眼几十年就没了。不知道你有有这没种感觉，随波逐流的那段时间，很多细节都想不起来，记忆非常模糊，有时候甚至会怀疑，那些日子是不是真的存在过。这个世界最大的幻觉就是稳定，所谓稳定，就是就是大脑幻想出来的确定性，事实上，人生唯一能确定的，就是世事难料。<br>&emsp;&emsp;如果你觉得你的生活很稳定，不需要任何努力就可以保持现状，只能说明你储在”最低能量状态“。山下的石头比山顶更稳定，仅仅是因为它的重力势能更低，处在自己的最低点。你可能会说，我比上不足比下有余，怎么可能在最低点，这里我说的是”自己”的最低点。佛说，众生平等，但不是众生相等。，每个人的最低点都不一样，你抱怨自己拼命追逐的只是别人的起点，但同样有很多人，终其一生，对着你的起点望尘莫及，这才是真正的众生平等。人和人是不一样的，人类的悲喜并不相同，不如意事常八九，可与言者无二三，没有人可以对他人感同身受，没有人可以对他人的人生负责。大伙都去吃唐僧肉，你也去了，结果猴子过来一看，要么是如来的灯芯，要么是老君的坐骑，迎面给了你一棒子。<br>&emsp;&emsp;人真正的转变就是不再从集体中寻求安全感，不在以”身边的人是怎么做的”当作行为依据，开始学会用逻辑和理性判断自己的做法是否合理，只相信自己的判断不在意别人的反馈。这种人会显得”自私“,但什么是自私，用自己喜欢的方式生活根本不叫自私，要求别人按自己喜欢的方式生活才叫自私。我们生来普通，却又生来不同，有所成就的人，绝不是和一群人扎堆，做同样的事，而是跳出来”群体陷阱“，专心做自己的事。牛顿第一定律告诉我们，保持匀速直线运动或者静止的物体最稳定。我们在说一个物体稳定的时候，实际上是在说，他没有”加速度“，甚至是没有速度静止不动，而加速度对应的是我们最宝贵的意志力和内驱力。失去人性，失去很多，失去兽性，失去一切。为什么历史上很多人只要打败一场回去就病死了，因为意志力一旦瓦解，生命力就会立刻凋零。巨鹿之战，项羽凭什么五万打四十万，一战封神，军事上有一个概念叫”崩溃率“。古代一般的军队伤亡率超过10%就会彻底崩溃，十万人只要伤亡一万，剩下的人就会变成疯狂逃串的野猪，战场就会变成对方的屠宰场，而项羽破釜沉舟，把过砸烂，把船凿沉，就是为了告诉大家，要么赢，要么死，没有”安稳“。<br>&emsp;&emsp;当是时，楚壮士无不以一当十，呼声震天。于是五万如何打赢四十万的问题就变成了如何歼敌四万和如何捕捉三十万的野猪。荥阳战，项羽三万人追着五十六万人打，歼敌近30万，差点杀了刘邦。垓下之战，乃有二十八骑，汉骑追者数千人，杀数百人，亡其两骑尔，项羽率领最后的二十八骑，斩敌数百，只损失两骑。剩余的二十六骑全部下马，跟随项羽英勇战死。这种战神虽然身死，但你很难说他败了，因为胜利的标准，不是杀光对方的士兵，而是彻底瓦解对方的战斗意志，但项羽的意志从来没有被瓦解过。生当作人杰，死亦为鬼雄，饮冰十年，难凉热血，你想不平庸，就一定有不平庸的办法，永远不要让自己的血凉下去，永远不要让自己心中执剑的少年混迹在市井之间，尤其是我们，后面还有好几十年，你可以成为任何你想成为的人，把自己变成一把钢锥，一辈子只凿一堵墙，肖申克的监狱也拦不住你。人生这场游戏只有一个秘诀，我是主角，我不能输。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随感 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
